{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasColas/ML01-Machine-Learning-for-everyone/blob/main/TD3_Naive_Bayes_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHxF0843b9ad"
      },
      "source": [
        "# ML01 : TD3 - Classifieur de Bayes empirique et Naïf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Emc1fvTub9ak"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import scipy.stats as stats\n",
        "\n",
        "np.random.seed(407)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59I87PEMb9ar"
      },
      "source": [
        "# 1. Contexte et introduction du Classifieur de Bayes Naïf\n",
        "\n",
        "Plaçons nous dans un contexte général où nous disposons d'un ensemble de $n$ observations labélisées (une base d'apprentissage contenant $n$ observations dont nous connaissons les classes). Notons $\\mathcal{S} = \\left\\{\\left(Y_i,X_{i}\\right), i = 1,\\dots,n\\right\\}$ cette base d'apprentissage et $K\\geq 2$ le nombre de classes possibles à diagnostiquer (à prédire). Pour chaque observation $i\\in\\{1,\\dots,n\\}$, \n",
        "- $X_{i} = [X_{i1},\\dots,X_{id}]$ correspondra au profil caractérisant l'observation $i$ ($X_{i}$ est un vecteur aléatoire composé de $d$ variables aléatoires).\n",
        "- $Y_{i}\\in\\{1,\\dots,K\\}$ correspondra à la variable aléatoire (catégorielle) caractérisant la classe de l'observation $i$.\n",
        "\n",
        "Nous nous plaçons de plus dans le contexte général où le modèles génératif ayant permis de générer les observations de la base d'apprentissage est inconnu. En d'autres termes, nous ne connaissons pas les densités de distributions des variables aléatoire $(Y_i,X_i)$.\n",
        "\n",
        "Notre objectif central reste le même que pour les précédents TDs : Nous souhaitons apprendre un classifieur (une règle de décision) à partir des données d'apprentissage permettant de prédire (diagnostiquer) la classe $Y_i$ d'une observation $i$ à partir du profil $X_i = [X_{i1},\\dots,X_{id}]$ caractérisant cette observation.\n",
        "Plus particulièrement dans ce TD, nous souhaitons apprendre le classifieur de Bayes Naïf.\n",
        "\n",
        "**Le classifieur de Bayes Naïf** est un modèle probabiliste charchant à approximer la formule de Bayes en supposant que toutes les variables explicatives $\\{X_{i1},\\dots,X_{id}\\}$, composant le profil $X_i$ de chaque observation, sont statistiquement indépendantes les unes des autres.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmra11xsb9ay"
      },
      "source": [
        "**Question 1.** Écrire la formule de Bayes permettant de modéliser la probabilité qu'une observation $i$ soit dans la classe $k\\in\\{1,\\dots,K\\}$ à partir du profile $X_i = [X_{i1},\\dots,X_{id}]$. En déduire la règle de décision du classifieur de Bayes permettant d'attribuer la classe la plus probable à une observation $X_i = x_i$ et simplifier là. Cette règle de décision simplifiée est communément appelée la \"MAP rule\" (règle de décision du Maximum A Posteriori).\n",
        "\n",
        "- Réponse à la Question 1 : \n",
        "\\begin{equation}\n",
        "        \\begin{aligned}\n",
        "            \\mathbb{P}(Y_i=k|X_i=x_i)  &\\;=\\; \\frac{\\mathbb{P}(Y_i=k) \\; \\mathbb{P}(X_{i}=x_i|Y_i=k)}{\\mathbb{P}(X_{i}=x_i)} \\\\\n",
        "             &\\;=\\; \\frac{\\mathbb{P}(Y_i=k) \\; \\mathbb{P}(X_{i1}=x_{i1},\\dots,X_{id}=x_{id}|Y_i=k)}{ \\mathbb{P}(X_{i1}=x_{i1},\\dots,X_{id}=x_{id}) } .\n",
        "            \\end{aligned}\n",
        "        \\end{equation}\n",
        "\n",
        "\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "        \\delta^B : x_i \\mapsto \\mathrm{argmax}_{k\\in\\{1,\\dots,K\\}} \\; \\mathbb{P}(Y_i=k|X_i=x_i)\n",
        "        \\end{equation}"
      ],
      "metadata": {
        "id": "KtY06ydEfHEB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pZPSmwRb9a1"
      },
      "source": [
        "**Question 1.2.** En supposant que toutes les variables explicatives $\\{X_{i1},\\dots,X_{id}\\}$ composant le profil $X_i$ de chaque observation sont indépendantes les unes des autres, comment pouvons-nous simplifier cette règle de décision du Maximum A Posteriori ?\n",
        "\n",
        "- Réponse à la Question 2 : \n",
        "\n",
        "\\begin{equation}\n",
        "            \\mathbb{P}(X_{i1},\\dots,X_{id}|Y_i=k) \\; = \\; \\prod_{j=1}^d \\mathbb{P}(X_{ij}|Y_i=k).\n",
        "    \\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "        \\begin{aligned}\n",
        "            \\mathbb{P}(Y_i=k|X_i=x_i)  &\\;=\\; \\frac{\\mathbb{P}(Y_i=k) \\; \\mathbb{P}(X_{i}=x_i|Y_i=k)}{\\mathbb{P}(X_{i}=x_i)} \\\\\n",
        "             &\\;=\\; \\frac{\\mathbb{P}(Y_i=k) \\;  \\prod_{j=1}^d \\mathbb{P}(X_{ij} = x_{ij}|Y_i=k)}{ \\prod_{j=1}^d \\mathbb{P}(X_{ij} = x_{ij}) } .\n",
        "            \\end{aligned}\n",
        "        \\end{equation}\n",
        "    La règle de décision du Maximum A Posteriori devient alors :\n",
        "    \\begin{equation}\n",
        "        \\delta^B : x_i \\mapsto \\mathrm{argmax}_{k\\in\\{1,\\dots,K\\}} \\; \\frac{\\mathbb{P}(Y_i=k) \\;  \\prod_{j=1}^d \\mathbb{P}(X_{ij} = x_{ij}|Y_i=k)}{ \\prod_{j=1}^d \\mathbb{P}(X_{ij} = x_{ij}) }\n",
        "    \\end{equation}\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn1tFe7lb9a4"
      },
      "source": [
        "**Intérêt de faire l'hypothèse d'indépendance entre variables.** Pour rappel, nous ne connaissons pas les densités de distributions des variables aléatoires $(Y_i,X_i)$. En particulier, nous ne connaissons pas la loi jointe $\\mathbb{P}(X_{i1},\\dots,X_{id}|Y_i=k)$ des variables caractéristiques dans chaque classe $k$ (également appelée la vraisemblance de chaque classe). Lorsque nous disposons d'un grand nombre $d$ de variables explicatives, l'estimation de cette la loi jointe $\\mathbb{P}(X_{i1},\\dots,X_{id}|Y_i=k)$ est très difficile (surtout lorsque nous travaillons avec des variables explicatives numériques (comme la température d'un patient) et d'autres catégorielles (comme l'état fumeur/non fumeur d'un patient). \n",
        "\n",
        "D'après la question 2, l'hypothèse naïve d'indépendance entre variables descrptives permet de simplifier cette étape de modélisation de la vraisemblance puisque cela nous ramène à modéliser indépendamment la vraisemblance $\\mathbb{P}(X_{ij}|Y_i=k)$ de chaque variable $j\\in\\{1,\\dots,d\\}$ dans chaque classe $k\\in\\{1,\\dots,K\\}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOsg_9wCb9a7"
      },
      "source": [
        "# 2. Apprentissage du classifieur de Bayes Naïf Empirique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNR_u8ORb9a8"
      },
      "source": [
        "D'après la question 2, nous avons besoin de modéliser indépendamment la vraisemblance $\\mathbb{P}(X_{ij}|Y_i=k)$ de chaque variable descriptive $j\\in\\{1,\\dots,d\\}$ dans chaque classe $k\\in\\{1,\\dots,K\\}$, ainsi que les probabilités à priori $\\mathbb{P}(Y_i=k)$ de chaque classe, le tout à partir des données d'apprentissage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZB4L9bLb9a-"
      },
      "source": [
        "**Question 2.1.** Concernant l'estimation des probabilités à priori de chaque classe, une approche usuelle est de les approximer en calculant les proportions par classe dans la base d'apprentissage. Autrement dit, les probabilités à priori $\\mathbb{P}(Y_i=k)$ de chaque classe $k\\in\\{1,\\dots,K\\}$ sont estimées par :\n",
        "$$\n",
        "\\hat{\\mathbb{P}}(Y_i=k) \\; = \\; \\frac{1}{n} \\sum_{i\\in\\mathcal{I}} \\mathbf{1}_{\\{Y_i = k\\}}.\n",
        "$$\n",
        "Implémenter une fonction permetant d'estimer les probabilités à priori $\\mathbb{P}(Y_i=k)$ de chaque classe $k\\in\\{1,\\dots,K\\}$ à partir des données d'apprentissage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbc0Ujfxb9bB"
      },
      "outputs": [],
      "source": [
        "# RÉPONSE À LA QUESTION 2.1 :\n",
        "\n",
        "# Nous appelerons \"priori_hat\" le vecteur contenant l'estimation des probabilités à priori de chaque classe.\n",
        "\n",
        "def estimation_priors(K,Y):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    K : int\n",
        "        Number of classes.\n",
        "    Y : np.array\n",
        "        Real labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    priori_hat : Array of floats\n",
        "        Class proportions.\n",
        "    '''\n",
        "    \n",
        "    priori_hat = np.zeros((1,K))\n",
        "    for i in range(K):\n",
        "      priori_hat[0,i] = np.sum(Y == i+1)/Y.shape[0]\n",
        "\n",
        "    \n",
        "    return priori_hat\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSHvIkmlb9bD"
      },
      "source": [
        "**Question 2.2.** Concernant la modélisation des vraisemblance $\\mathbb{P}(X_{ij}|Y_i=k)$ de chaque variable descriptive $j\\in\\{1,\\dots,d\\}$ dans chaque classe, une approche usuelle est la suivante :\n",
        "\n",
        "- Si la variable descriptive $j$ est une variable catégorielle à $T\\geq 2$ valeurs possibles (par exemple est-ce qu'un patient est fumeur ou non), nous considérons que $(X_{ij}|Y_i = k) \\sim \\mathrm{Cat}(T,[p_{j,k,1},\\dots,p_{j,k,T}])$, où quelque soit $t \\in \\{1,\\dots,T\\}$,   $p_{j,k,t} = \\mathbb{P}(X_{ij} = t|Y_i=k)$.\n",
        "  Comme nous l'avons rappelé dans la première partie, la loi de $(X_{ij}|Y_i = k)$ n'est pas connue est donc nous ne connaissons pas les paramètres $[p_{j,k,1},\\dots,p_{j,k,T}]$. À partir de la base d'apprentissage, nous estimons donc ces paramètres de sorte que nos paramètres estimés rendent les observations de la base d'apprentissage les plus plausibles possible. Autrement dit nous les estimons par la méthode du maximum de vraisemblance. Ceci revient à calculer, pour chaque variable catégorielle $j$, pour chaque classe $k\\in\\{1,\\dots,K\\}$ et pour chaque catégorie $t \\in \\{1,\\dots,T\\}$ :\n",
        "  \n",
        "\n",
        "$$  \n",
        "  \\hat{p}_{j,k,t} \\; = \\; \\hat{\\mathbb{P}}(X_{ij} = t \\mid Y_i=k) \\; = \\; \\frac{1}{n_k} \\sum_{i\\in\\mathcal{I_k}} \\mathbf{1}_{\\{x_{ij} = t\\}},\n",
        "$$\n",
        "\n",
        "\n",
        "  \n",
        "où $I_k$ correspond aux observations de la base d'apprentissage issues de la classe $k$ et $n_k$ correspond au nombre des ces observations issues de la classe $k$.\n",
        "  \n",
        "\n",
        "- Si la variable descriptive $j$ est une variable numérique, une approche usuelle est alors de modéliser les distributions de $(X_{ij}|Y_i = k)$ par des gaussiennes dont les moyennes et variances sont à estimer à partir de la base d'apprentissage, en cherchant à maximiser la vraisemblance par classe. Autrement dit, on estime que pour chaque variable numérique $j$ et pour chaque classe $k\\in\\{1,\\dots,K\\}$, $(X_{ij}|Y_i = k) \\sim \\mathcal{N}(\\hat{\\mu}_{jk},\\hat{\\sigma}_{jk}^2)$, où d'après la question 3 du TD2, les paramètres $\\left\\{\\hat{\\mu}_{jk},\\hat{\\sigma}_{jk}^2\\right\\}$ sont estimés par\n",
        "\n",
        "$$\n",
        "\\hat{\\mu}_{jk} = \\frac{1}{n_k} \\sum_{i\\in I_k} x_{ij} \n",
        "\\quad \\quad \\text{et} \\quad \\quad\n",
        "\\hat{\\sigma}_{jk} = \\frac{1}{n_k} \\sum_{i\\in I_k} \\left(x_{ij} - \\hat{\\mu}_{jk} \\right)^2.\n",
        "$$\n",
        "    \n",
        "  De ce fait, la vraisemblance $\\hat{\\mathbb{P}}(X_{ij} = x_{ij} \\mid Y_i=k)$ de chaque variable descriptive numérique $j$ dans chaque classe $k$ est estimée par\n",
        "  \n",
        "$$\n",
        "    \\hat{\\mathbb{P}}(X_{ij} = x_{ij} \\mid Y_i=k) \\;=\\; \\frac{1}{\\hat{\\sigma}_{jk} \\sqrt{2\\pi}} \\, \\mathrm{exp}\\left( -\\frac{(x_{ij}-\\hat{\\mu}_{jk})^2}{2\\hat{\\sigma}_{jk}^2}\\right).\n",
        "$$\n",
        "    \n",
        "    \n",
        "- Question : Implémenter une fonction permettant d'estimer les paramètres des vraisemblances d'une variable catégorielle $j$ dans chaque classe $k$. Implémenter ensuite une fonction permettant d'estimer les paramètres des vraisemblances d'une variable numérique $j$ dans chaque classe $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "leJXb-yTb9bG"
      },
      "outputs": [],
      "source": [
        "# RÉPONSE À LA QUESTION 2.2 :\n",
        "\n",
        "# Pour une variable catégorielle :\n",
        "\n",
        "def estimation_vraisemblance_cat(Xj,Y,K,T):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    Xj : np.array\n",
        "        Vecteur contenant les données d'apprentissage de la variable j.\n",
        "    Y : np.array\n",
        "        Labels des observations de la base d'apprentissage\n",
        "    K : int\n",
        "        Number of classes.\n",
        "    T : int\n",
        "        Number of discrete profiles for the categorical variable Xj.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pHatj : Array of floats\n",
        "        Estimation des vraisemblance catégorielles dans chaque classe.\n",
        "    '''\n",
        "    \n",
        "    pHatj = np.zeros((K,T))\n",
        "    \n",
        "    # .... À COMPLETER. ...\n",
        "    for k in range(K):\n",
        "      nk = np.sum(Y==k+1)\n",
        "      Ik = np.where(Y == k+1)\n",
        "      for t in range(T):\n",
        "            pHatj[k,t] = np.sum(Xj[Ik[0]]==t)/nk\n",
        "    \n",
        "    return pHatj\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "# Pour une variable numérique :\n",
        "\n",
        "def estimation_vraisemblance_num(Xj,Y,K):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    Xj : np.array\n",
        "        Vecteur contenant les données d'apprentissage de la variable j.\n",
        "    Y : np.array\n",
        "        Labels des observations de la base d'apprentissage\n",
        "    K : int\n",
        "        Number of classes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mu_j : Array of floats\n",
        "        Estimation des moyennes des gaussiennes dans chaque classe.\n",
        "    sigma_j : Array of floats\n",
        "        Estimation des écart-types des gaussiennes dans chaque classe.\n",
        "    '''\n",
        "\n",
        "    mu_j = np.zeros((1,K))\n",
        "    sigma_j = np.zeros((1,K))\n",
        "    \n",
        "    # .... À COMPLETER. ...\n",
        "    for k in range(K):\n",
        "      Ik = np.where(Y==(k+1))\n",
        "      mu_j[0,k] = np.mean(Xj[Ik[0]])\n",
        "      sigma_j[0,k] = np.std(Xj[Ik[0]])\n",
        "\n",
        "    \n",
        "    return mu_j, sigma_j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEsTjkw-b9bI"
      },
      "source": [
        "# 3. Application du classifieur de Bayes Naïf Empirique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kybDsFcb9bJ"
      },
      "source": [
        "Compiler la cellule ci-dessous permettant de générer une base de données d'apprentissage contenant nTrain = 100 observations et une base de test contenant 900 observations. \n",
        "- Chaque observation est décrite par 3 variables : les deux premières variables sont numériques et la 3ème variable est une variable catégorielle avec $T=4$ catégories possibles.\n",
        "- Nous avons $K=2$ classes à prédire à partir de ces 3 variables descriptives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BGMoT63Lb9bK"
      },
      "outputs": [],
      "source": [
        "# Génération base de données\n",
        "\n",
        "# RÉPONSE À LA QUESTION 1:\n",
        "\n",
        "np.random.seed(407)\n",
        "\n",
        "K = 2\n",
        "\n",
        "mu11 = 37.8\n",
        "mu12 = 39\n",
        "sigma11 = 0.4\n",
        "sigma12 = 0.3\n",
        "\n",
        "mu21 = 7\n",
        "mu22 = 8\n",
        "sigma21 = 0.7\n",
        "sigma22 = 1\n",
        "\n",
        "T = 4\n",
        "pHat = np.zeros((K,T))\n",
        "pHat[0,0]=0.1 \n",
        "pHat[0,1]=0.2 \n",
        "pHat[0,2]=0.5 \n",
        "pHat[0,3]=1-(pHat[0,0]+pHat[0,1]+pHat[0,2]) \n",
        "pHat[1,0]=0.3 \n",
        "pHat[1,1]=0.1 \n",
        "pHat[1,2]=0.2 \n",
        "pHat[1,3]=1-(pHat[1,0]+pHat[1,1]+pHat[1,2])     \n",
        "\n",
        "priori_2 = 0.3\n",
        "\n",
        "\n",
        "# Génération de la base d'apprentissage :\n",
        "\n",
        "nTrain = 100\n",
        "YTrain = np.zeros((nTrain,1))\n",
        "XTrain = np.zeros((nTrain,3))\n",
        "\n",
        "for i in range(0,nTrain):\n",
        "    YTrain[i,0] = int(np.random.binomial(1, priori_2, 1) + 1)\n",
        "    if YTrain[i,0]==1:\n",
        "        XTrain[i,0] = np.random.normal(mu11, sigma11, 1)\n",
        "        XTrain[i,1] = np.random.normal(mu21, sigma21, 1)\n",
        "        XTrain[i,2] = np.random.choice(T, 1, p=pHat[0]) \n",
        "    if YTrain[i,0]==2:\n",
        "        XTrain[i,0] = np.random.normal(mu12, sigma12, 1)\n",
        "        XTrain[i,1] = np.random.normal(mu22, sigma22, 1)\n",
        "        XTrain[i,2] = np.random.choice(T, 1, p=pHat[1])\n",
        "    \n",
        "\n",
        "# Génération de la base de test :\n",
        "    \n",
        "nTest= 900\n",
        "YTest = np.zeros((nTest,1))\n",
        "XTest = np.zeros((nTest,3))\n",
        "\n",
        "for i in range(0,nTest):\n",
        "    YTest[i,0] = int(np.random.binomial(1, priori_2, 1) + 1)\n",
        "    if YTest[i,0]==1:\n",
        "        XTest[i,0] = np.random.normal(mu11, sigma11, 1)\n",
        "        XTest[i,1] = np.random.normal(mu21, sigma21, 1)\n",
        "        XTest[i,2] = np.random.choice(T, 1, p=pHat[0]) \n",
        "    if YTest[i,0]==2:\n",
        "        XTest[i,0] = np.random.normal(mu12, sigma12, 1)\n",
        "        XTest[i,1] = np.random.normal(mu22, sigma22, 1)\n",
        "        XTest[i,2] = np.random.choice(T, 1, p=pHat[1])\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSlgB7fmb9bM"
      },
      "source": [
        "**Question 3.1.** Estimer sur la base d'apprentissage les paramètres introduits dans la section 2, pour chaque variable indépendamment, à partir des trois fonctions implémentées lors des questions 2.1 et 2.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tvM1W4TKb9bN"
      },
      "outputs": [],
      "source": [
        "# RÉPONSE À LA QUESTION 3.1 :\n",
        "\n",
        "# .... À COMPLETER. ...\n",
        "# Estimation des probabilités à priori :\n",
        "priori_hat = estimation_priors(K,YTrain)\n",
        "\n",
        "# Estimation des paramètres des vraisemblances de chaque classe pour la variable 1 qui est numérique :\n",
        "XTrain_1 = XTrain[:,0]\n",
        "mu_1, sigma_1 = estimation_vraisemblance_num(XTrain_1,YTrain,K)\n",
        "\n",
        "# Estimation des paramètres des vraisemblances de chaque classe pour la variable 2 qui est numérique :\n",
        "XTrain_2 = XTrain[:,1]\n",
        "mu_2, sigma_2 = estimation_vraisemblance_num(XTrain_2,YTrain,K)\n",
        "\n",
        "# Estimation des paramètres des vraisemblances de chaque classe pour la variable 3 qui est catégorielle :\n",
        "XTrain_3 = XTrain[:,2]\n",
        "pHat3 = estimation_vraisemblance_cat(XTrain_3,YTrain,K,T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R6HWMUdb9bP"
      },
      "source": [
        "**Question 3.2.** Appliquer le classifieur de Bayes Naïf sur les observations de la base de test et calculer le taux d'erreur global de ce classifieur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zj5RZWO4b9bP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd572da-3565-42f9-9839-3a636b7b2f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taux erreur global du classifieur de Bayes Naif sur la base de test = 0.03888888888888889\n"
          ]
        }
      ],
      "source": [
        "# RÉPONSE À LA QUESTION 3.2 :\n",
        "\n",
        "YhatTest = np.zeros((nTest,1))\n",
        "\n",
        "for i in range(nTest):\n",
        "    \n",
        "    # Estimation des probabilités à posteriori dans chaque classe :\n",
        "    Posteriori = np.zeros((1,K))\n",
        "    for k in range(K):\n",
        "        \n",
        "        pX1k_xi = 1/(sigma_1[0,k]*np.sqrt(2*np.pi)) * np.exp(-(XTest[i,0] - mu_1[0,k])**2/(2*sigma_1[0,k]**2))\n",
        "        pX2k_xi = 1/(sigma_2[0,k]*np.sqrt(2*np.pi)) * np.exp(-(XTest[i,1] - mu_2[0,k])**2/(2*sigma_2[0,k]**2))\n",
        "        pX3k_xi = pHat3[k,int(XTest[i,2])]\n",
        "        \n",
        "        Posteriori[0,k] = priori_hat[0,k] * pX1k_xi * pX2k_xi * pX3k_xi\n",
        "        \n",
        "    # Règle de décision Maximum A Posteriori (MAP rule) pour prédire la classe :\n",
        "    YhatTest[i] = np.argmax(Posteriori)+1\n",
        "    \n",
        "\n",
        "# Calcul du taux d'erreur global :\n",
        "errGlobalTest = np.sum(YhatTest!=YTest)/nTest\n",
        "print('Taux erreur global du classifieur de Bayes Naif sur la base de test =', errGlobalTest)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH1QJ7IMb9bQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}